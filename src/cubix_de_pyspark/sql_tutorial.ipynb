{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6798473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DateType,\n",
    "    TimestampType\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('SQL')\n",
    "    .master('local[*]')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6239d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_2024_09 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"parquet\")\n",
    "    .load('../cubix_de_pyspark/data/yellow_tripdata_2024-09.parquet')\n",
    ")\n",
    "\n",
    "taxi_data_2024_09.createOrReplaceTempView(\"taxi_2024_09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f55f1c",
   "metadata": {},
   "source": [
    "#### SQL recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-09-01 00:05:51|  2024-09-01 00:45:03|              1|          9.8|         1|                 N|         138|          48|           1|       47.8|10.25|    0.5|      13.3|        6.94|                  1.0|       79.79|                 2.5|       1.75|\n",
      "|       1| 2024-09-01 00:59:35|  2024-09-01 01:03:43|              1|          0.5|         1|                 N|         140|         141|           1|        5.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        13.1|                 2.5|        0.0|\n",
      "|       2| 2024-09-01 00:25:00|  2024-09-01 00:34:37|              2|         2.29|         1|                 N|         238|         152|           2|       13.5|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.0|                 0.0|        0.0|\n",
      "|       2| 2024-09-01 00:31:00|  2024-09-01 00:46:52|              1|          5.2|         1|                 N|          93|         130|           1|       24.7|  1.0|    0.5|      4.55|         0.0|                  1.0|       31.75|                 0.0|        0.0|\n",
      "|       2| 2024-09-01 00:11:57|  2024-09-01 00:30:41|              2|         2.26|         1|                 N|          79|         231|           1|       17.0|  1.0|    0.5|       4.4|         0.0|                  1.0|        26.4|                 2.5|        0.0|\n",
      "|       1| 2024-09-01 00:30:13|  2024-09-01 00:36:44|              1|          1.2|         1|                 N|          43|         239|           1|        8.6|  3.5|    0.5|       2.7|         0.0|                  1.0|        16.3|                 2.5|        0.0|\n",
      "|       1| 2024-09-01 00:59:24|  2024-09-01 01:01:00|              1|          0.1|         5|                 N|         143|         143|           3|       0.01|  0.0|    0.0|       0.0|         0.0|                  1.0|        1.01|                 0.0|        0.0|\n",
      "|       1| 2024-09-01 00:08:28|  2024-09-01 00:39:06|              4|          9.8|         1|                 N|          93|         161|           1|       44.3|  3.5|    0.5|      9.85|         0.0|                  1.0|       59.15|                 2.5|        0.0|\n",
      "|       1| 2024-09-01 00:06:07|  2024-09-01 00:11:38|              1|          0.6|         1|                 N|         170|         137|           1|        6.5|  3.5|    0.5|       2.9|         0.0|                  1.0|        14.4|                 2.5|        0.0|\n",
      "|       1| 2024-09-01 00:20:28|  2024-09-01 00:36:06|              1|          4.1|         1|                 N|          79|         263|           1|       19.1|  3.5|    0.5|       4.8|         0.0|                  1.0|        28.9|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from taxi_2024_09\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------------+\n",
      "|VendorID|passenger_count|total_amount|\n",
      "+--------+---------------+------------+\n",
      "|       2|              8|       -93.0|\n",
      "|       2|              7|       -74.0|\n",
      "|       2|              7|        40.8|\n",
      "|       2|              6|       19.68|\n",
      "|       2|              6|       11.76|\n",
      "|       2|              6|        20.5|\n",
      "|       2|              6|       31.08|\n",
      "|       2|              6|       23.16|\n",
      "|       2|              6|       21.84|\n",
      "|       2|              6|        21.2|\n",
      "+--------+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          passenger_count,\n",
    "          total_amount\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "WHERE\n",
    "          passenger_count > 4\n",
    "          AND total_amount < 50\n",
    "ORDER BY\n",
    "          passenger_count DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------------+\n",
      "|VendorID|max_passenger_count|  avg_total_amount|\n",
      "+--------+-------------------+------------------+\n",
      "|       2|                  9|28.754969323884783|\n",
      "|       6|               NULL| 32.86155963302752|\n",
      "+--------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "          VendorID,\n",
    "          max(passenger_count) AS max_passenger_count,\n",
    "          avg(total_amount) AS avg_total_amount\n",
    "    FROM\n",
    "          taxi_2024_09\n",
    "    GROUP BY\n",
    "          VendorID\n",
    "    HAVING\n",
    "          avg_total_amount > 28\n",
    "\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b87a3",
   "metadata": {},
   "source": [
    "#### Advanced concepts\n",
    "\n",
    "##### Handling dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commas needed after values inside ()\n",
    "data = [\n",
    "    (\"12/01/2023\",),\n",
    "    (\"11/15/2023\",),\n",
    "    (\"10/25/2023\",),\n",
    "    (\"09/17/2023\",)\n",
    "]\n",
    "\n",
    "schema = StructType([StructField(\"date_string\", StringType(), True)])\n",
    "\n",
    "date_df = spark.createDataFrame(data, schema)\n",
    "date_df.createOrReplaceTempView(\"dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42015f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|date_string|parsed_date|\n",
      "+-----------+-----------+\n",
      "| 12/01/2023| 2023-12-01|\n",
      "| 11/15/2023| 2023-11-15|\n",
      "| 10/25/2023| 2023-10-25|\n",
      "| 09/17/2023| 2023-09-17|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert string to DATE to to be able to use date functions\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "          date_string,\n",
    "          TO_DATE(date_string, 'MM/dd/yyyy') AS parsed_date\n",
    "    FROM\n",
    "          dates\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+---+-------------+---------+\n",
      "|parsed_date|year|month|day|weekday_short|  weekday|\n",
      "+-----------+----+-----+---+-------------+---------+\n",
      "| 2023-12-01|2023|   12|  1|          Fri|   Friday|\n",
      "| 2023-11-15|2023|   11| 15|          Wed|Wednesday|\n",
      "| 2023-10-25|2023|   10| 25|          Wed|Wednesday|\n",
      "| 2023-09-17|2023|    9| 17|          Sun|   Sunday|\n",
      "+-----------+----+-----+---+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extracting parts from date\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "          TO_DATE(date_string, 'MM/dd/yyyy') AS parsed_date,\n",
    "          YEAR(TO_DATE(date_string, 'MM/dd/yyyy')) AS year,\n",
    "          MONTH(TO_DATE(date_string, 'MM/dd/yyyy')) AS month,\n",
    "          DAY(TO_DATE(date_string, 'MM/dd/yyyy')) AS day,\n",
    "          DATE_FORMAT(TO_DATE(date_string, 'MM/dd/yyyy'), 'E') AS weekday_short,\n",
    "          DATE_FORMAT(TO_DATE(date_string, 'MM/dd/yyyy'), 'EEEE') AS weekday\n",
    "    FROM\n",
    "          dates\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------------+------------+----------------+----------+\n",
      "|parsed_date|add_10_days|subtract_5_days|add_2_months|subtract_1_month|add_1_year|\n",
      "+-----------+-----------+---------------+------------+----------------+----------+\n",
      "| 2023-12-01| 2023-12-11|     2023-11-26|  2024-02-01|      2023-11-01|2024-12-01|\n",
      "| 2023-11-15| 2023-11-25|     2023-11-10|  2024-01-15|      2023-10-15|2024-11-15|\n",
      "| 2023-10-25| 2023-11-04|     2023-10-20|  2023-12-25|      2023-09-25|2024-10-25|\n",
      "| 2023-09-17| 2023-09-27|     2023-09-12|  2023-11-17|      2023-08-17|2024-09-17|\n",
      "+-----------+-----------+---------------+------------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding and removing dates\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "          TO_DATE(date_string, 'MM/dd/yyyy') AS parsed_date,\n",
    "          DATE_ADD(TO_DATE(date_string, 'MM/dd/yyyy'), 10) AS add_10_days,\n",
    "          DATE_SUB(TO_DATE(date_string, 'MM/dd/yyyy'), 5) AS subtract_5_days,\n",
    "          ADD_MONTHS(TO_DATE(date_string, 'MM/dd/yyyy'), 2) AS add_2_months,\n",
    "          ADD_MONTHS(TO_DATE(date_string, 'MM/dd/yyyy'), -1) AS subtract_1_month,\n",
    "          ADD_MONTHS(TO_DATE(date_string, 'MM/dd/yyyy'), 12) AS add_1_year\n",
    "    FROM\n",
    "          dates\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccc099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|date_string|parsed_date|\n",
      "+-----------+-----------+\n",
      "| 12/01/2023| 2023-12-01|\n",
      "| 11/15/2023| 2023-11-15|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using BETWEEN and AND\n",
    "\n",
    "# filtering variable in WHERE must be in date format if we filter dates!\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "          date_string,\n",
    "          TO_DATE(date_string, 'MM/dd/yyyy') AS parsed_date\n",
    "    FROM\n",
    "          dates\n",
    "    WHERE\n",
    "          TO_DATE(date_string, 'MM/dd/yyyy') BETWEEN '2023-11-01' AND '2023-12-31'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------------+\n",
      "|parsed_date|     today|days_difference|\n",
      "+-----------+----------+---------------+\n",
      "| 2023-12-01|2025-11-26|            726|\n",
      "| 2023-11-15|2025-11-26|            742|\n",
      "| 2023-10-25|2025-11-26|            763|\n",
      "| 2023-09-17|2025-11-26|            801|\n",
      "+-----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# difference between dates with CTE\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    WITH date_diff_example AS (\n",
    "        SELECT\n",
    "            TO_DATE(date_string, 'MM/dd/yyyy') AS parsed_date,\n",
    "            CURRENT_DATE() AS today\n",
    "        FROM\n",
    "            dates\n",
    "    )\n",
    "    SELECT \n",
    "        parsed_date,\n",
    "        today,\n",
    "        DATEDIFF(today, parsed_date) AS days_difference\n",
    "    FROM\n",
    "        date_diff_example\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0053f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|date_column|   timestamp_column|\n",
      "+-----------+-------------------+\n",
      "| 2024-12-31|2024-12-31 23:59:59|\n",
      "| 2023-01-01|2023-01-01 10:15:30|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# timestamps\n",
    "\n",
    "data = [\n",
    "    (date(2024, 12, 31), datetime(2024, 12, 31, 23, 59, 59)),\n",
    "    (date(2023, 1, 1), datetime(2023, 1, 1, 10, 15, 30))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_column\", DateType(), True),\n",
    "    StructField(\"timestamp_column\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"timestamps\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----+------+------+-------------------+\n",
      "|   timestamp_column|parsed_date|hour|minute|second|     truncated_hour|\n",
      "+-------------------+-----------+----+------+------+-------------------+\n",
      "|2024-12-31 23:59:59| 2024-12-31|  23|    59|    59|2024-12-31 23:00:00|\n",
      "|2023-01-01 10:15:30| 2023-01-01|  10|    15|    30|2023-01-01 10:00:00|\n",
      "+-------------------+-----------+----+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# timestamp functions\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "          timestamp_column,\n",
    "          TO_DATE(timestamp_column, 'MM/dd/yyyy') AS parsed_date,\n",
    "          HOUR(timestamp_column) AS hour,\n",
    "          MINUTE(timestamp_column) AS minute,\n",
    "          SECOND(timestamp_column) AS second,\n",
    "          DATE_TRUNC('HOUR', timestamp_column) AS truncated_hour\n",
    "    FROM\n",
    "          timestamps\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c5b8e",
   "metadata": {},
   "source": [
    "##### Union/Union all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb50527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF1:\n",
      "+---+---------+---+------+\n",
      "| id|     name|age|gender|\n",
      "+---+---------+---+------+\n",
      "|  1|    Alice| 25|     F|\n",
      "|  2|      Bob| 17|     M|\n",
      "|  3|Catherine| 40|     F|\n",
      "+---+---------+---+------+\n",
      "\n",
      "DF2:\n",
      "+---+---------+---+------+\n",
      "| id|     name|age|gender|\n",
      "+---+---------+---+------+\n",
      "|  3|Catherine| 40|     F|\n",
      "|  4|    David| 15|     M|\n",
      "|  5|      Eva| 29|     F|\n",
      "+---+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (1, \"Alice\", 25, \"F\"),\n",
    "    (2, \"Bob\", 17, \"M\"),\n",
    "    (3, \"Catherine\", 40, \"F\")\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "    (3, \"Catherine\", 40, \"F\"),\n",
    "    (4, \"David\", 15, \"M\"),\n",
    "    (5, \"Eva\", 29, \"F\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"gender\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "\n",
    "print(\"DF1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"DF2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2300a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+------+\n",
      "| id|     name|age|gender|\n",
      "+---+---------+---+------+\n",
      "|  1|    Alice| 25|     F|\n",
      "|  2|      Bob| 17|     M|\n",
      "|  3|Catherine| 40|     F|\n",
      "|  4|    David| 15|     M|\n",
      "|  5|      Eva| 29|     F|\n",
      "+---+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of columns and data types must be the same\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM df1\n",
    "    UNION\n",
    "    SELECT * FROM df2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b04499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+------+\n",
      "| id|     name|age|gender|\n",
      "+---+---------+---+------+\n",
      "|  1|    Alice| 25|     F|\n",
      "|  2|      Bob| 17|     M|\n",
      "|  3|Catherine| 40|     F|\n",
      "|  3|Catherine| 40|     F|\n",
      "|  4|    David| 15|     M|\n",
      "|  5|      Eva| 29|     F|\n",
      "+---+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM df1\n",
    "    UNION ALL\n",
    "    SELECT * FROM df2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79adc992",
   "metadata": {},
   "source": [
    "##### Case when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+------+\n",
      "| id|     name|age|gender|\n",
      "+---+---------+---+------+\n",
      "|  1|    Alice| 25|     F|\n",
      "|  2|      Bob| 17|     M|\n",
      "|  3|Catherine| 40|     F|\n",
      "|  4|    David| 15|     M|\n",
      "|  5|      Eva| 29|     F|\n",
      "+---+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 25, \"F\"),\n",
    "    (2, \"Bob\", 17, \"M\"),\n",
    "    (3, \"Catherine\", 40, \"F\"),\n",
    "    (4, \"David\", 15, \"M\"),\n",
    "    (5, \"Eva\", 29, \"F\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"gender\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddf1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+\n",
      "| id|     name|age|age_group|\n",
      "+---+---------+---+---------+\n",
      "|  1|    Alice| 25|    Adult|\n",
      "|  2|      Bob| 17|    Minor|\n",
      "|  3|Catherine| 40|    Adult|\n",
      "|  4|    David| 15|    Minor|\n",
      "|  5|      Eva| 29|    Adult|\n",
      "+---+---------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT\n",
    "          id,\n",
    "          name,\n",
    "          age,\n",
    "          CASE\n",
    "            WHEN age >= 18 THEN 'Adult'\n",
    "            ELSE 'Minor'\n",
    "          END AS age_group\n",
    "    FROM people\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+------------+\n",
      "| id|     name|gender| eligibility|\n",
      "+---+---------+------+------------+\n",
      "|  1|    Alice|     F|    Eligible|\n",
      "|  2|      Bob|     M|Not eligible|\n",
      "|  3|Catherine|     F|    Eligible|\n",
      "|  4|    David|     M|Not eligible|\n",
      "|  5|      Eva|     F|    Eligible|\n",
      "+---+---------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT\n",
    "          id,\n",
    "          name,\n",
    "          gender,\n",
    "          CASE\n",
    "            WHEN gender == 'F' THEN 'Eligible'\n",
    "            ELSE 'Not eligible'\n",
    "          END AS eligibility\n",
    "    FROM people\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6517024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+------------+\n",
      "| id|     name|age|    category|\n",
      "+---+---------+---+------------+\n",
      "|  1|    Alice| 25|Adult Female|\n",
      "|  2|      Bob| 17|       Minor|\n",
      "|  3|Catherine| 40|Adult Female|\n",
      "|  4|    David| 15|       Minor|\n",
      "|  5|      Eva| 29|Adult Female|\n",
      "+---+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT\n",
    "          id,\n",
    "          name,\n",
    "          age,\n",
    "          CASE\n",
    "            WHEN age >= 18 and GENDER == 'F' THEN 'Adult Female'\n",
    "            WHEN age >= 18 and GENDER == 'M' THEN 'Adult Male'\n",
    "          ELSE 'Minor'\n",
    "          END AS category\n",
    "    FROM people\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f8a02",
   "metadata": {},
   "source": [
    "##### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15e039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+------+\n",
      "| id|     name| age|gender|\n",
      "+---+---------+----+------+\n",
      "|  1|    Alice|  25|     F|\n",
      "|  2|     NULL|  17|     M|\n",
      "|  3|Catherine|NULL|     F|\n",
      "|  4|    David|  15|     M|\n",
      "|  5|      Eva|  29|     F|\n",
      "+---+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 25, \"F\"),\n",
    "    (2, None, 17, \"M\"),\n",
    "    (3, \"Catherine\", None, \"F\"),\n",
    "    (4, \"David\", 15, \"M\"),\n",
    "    (5, \"Eva\", 29, \"F\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"gender\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------------+------+\n",
      "| id|coalesce(name, unknown)|coalesce(age, 0)|gender|\n",
      "+---+-----------------------+----------------+------+\n",
      "|  1|                  Alice|              25|     F|\n",
      "|  2|                unknown|              17|     M|\n",
      "|  3|              Catherine|               0|     F|\n",
      "|  4|                  David|              15|     M|\n",
      "|  5|                    Eva|              29|     F|\n",
      "+---+-----------------------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT\n",
    "          id,\n",
    "          COALESCE(name, \"unknown\"),\n",
    "          COALESCE(age, 0),\n",
    "          gender\n",
    "    FROM people\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8893c",
   "metadata": {},
   "source": [
    "##### Subquery and CTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a63178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------+----------+\n",
      "|employee_id|employee_name| department|salary| hire_date|\n",
      "+-----------+-------------+-----------+------+----------+\n",
      "|          1|        Alice|         HR| 55000|2020-01-01|\n",
      "|          2|          Bob|Engineering| 80000|2020-01-01|\n",
      "|          3|      Charlie|         HR| 60000|2019-05-15|\n",
      "|          4|        David|Engineering| 95000|2018-07-01|\n",
      "|          5|          Eva|  Marketing| 70000|2022-09-10|\n",
      "|          6|        Frank|  Marketing| 72000|2021-12-05|\n",
      "|          7|        Grace|Engineering| 85000|2019-11-23|\n",
      "|          8|         Hank|         HR| 63000|2020-11-11|\n",
      "+-----------+-------------+-----------+------+----------+\n",
      "\n",
      "+-------------+---------------+--------------------+\n",
      "|department_id|department_code|     department_name|\n",
      "+-------------+---------------+--------------------+\n",
      "|            1|             HR|     Human Resources|\n",
      "|            2|    Engineering|Software Engineering|\n",
      "|            3|      Marketing|Marketing Department|\n",
      "+-------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (1, \"Alice\", \"HR\", 55000, \"2020-01-01\"),\n",
    "    (2, \"Bob\", \"Engineering\", 80000, \"2020-01-01\"),\n",
    "    (3, \"Charlie\", \"HR\", 60000, \"2019-05-15\"),\n",
    "    (4, \"David\", \"Engineering\", 95000, \"2018-07-01\"),\n",
    "    (5, \"Eva\", \"Marketing\", 70000, \"2022-09-10\"),\n",
    "    (6, \"Frank\", \"Marketing\", 72000, \"2021-12-05\"),\n",
    "    (7, \"Grace\", \"Engineering\", 85000, \"2019-11-23\"),\n",
    "    (8, \"Hank\", \"HR\", 63000, \"2020-11-11\")\n",
    "]\n",
    "\n",
    "employee_columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\", \"hire_date\"]\n",
    "\n",
    "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
    "employee_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "employee_df.show()\n",
    "\n",
    "departments_data = [\n",
    "    (1, \"HR\", \"Human Resources\"),\n",
    "    (2, \"Engineering\", \"Software Engineering\"),\n",
    "    (3, \"Marketing\", \"Marketing Department\")\n",
    "]\n",
    "\n",
    "departments_columns = [\"department_id\", \"department_code\", \"department_name\"]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, departments_columns)\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ca7fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------+\n",
      "|employee_id|employee_name| department|salary|\n",
      "+-----------+-------------+-----------+------+\n",
      "|          3|      Charlie|         HR| 60000|\n",
      "|          4|        David|Engineering| 95000|\n",
      "|          6|        Frank|  Marketing| 72000|\n",
      "|          8|         Hank|         HR| 63000|\n",
      "+-----------+-------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   SELECT\n",
    "          employee_id,\n",
    "          employee_name,\n",
    "          department,\n",
    "          salary\n",
    "    FROM \n",
    "          employees e\n",
    "    WHERE\n",
    "          salary > (\n",
    "            SELECT AVG(salary)\n",
    "            FROM employees\n",
    "            WHERE department = e.department\n",
    "          )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cf3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------+\n",
      "|employee_id|employee_name| department|salary|\n",
      "+-----------+-------------+-----------+------+\n",
      "|          3|      Charlie|         HR| 60000|\n",
      "|          4|        David|Engineering| 95000|\n",
      "|          6|        Frank|  Marketing| 72000|\n",
      "|          8|         Hank|         HR| 63000|\n",
      "+-----------+-------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          \n",
    "    WITH avg_salaries AS (\n",
    "            SELECT department, AVG(salary) AS avg_salary\n",
    "            FROM employees\n",
    "            GROUP BY department\n",
    "    )\n",
    "    SELECT\n",
    "          e.employee_id,\n",
    "          e.employee_name,\n",
    "          e.department,\n",
    "          e.salary\n",
    "    FROM \n",
    "          employees e\n",
    "    JOIN\n",
    "          avg_salaries a\n",
    "          ON e.department = a.department\n",
    "    WHERE\n",
    "          e.salary > a.avg_salary\n",
    "    \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0ecac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+--------------------+\n",
      "|employee_id|employee_name|salary|     department_name|\n",
      "+-----------+-------------+------+--------------------+\n",
      "|          4|        David| 95000|Software Engineering|\n",
      "|          3|      Charlie| 60000|     Human Resources|\n",
      "|          8|         Hank| 63000|     Human Resources|\n",
      "|          6|        Frank| 72000|Marketing Department|\n",
      "+-----------+-------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          \n",
    "    WITH avg_salaries AS (\n",
    "            SELECT department, AVG(salary) AS avg_salary\n",
    "            FROM employees\n",
    "            GROUP BY department\n",
    "    ),\n",
    "    employee_info AS (\n",
    "            SELECT e.employee_id, e.employee_name, e.department, e.salary\n",
    "            FROM employees e     \n",
    "    )\n",
    "    SELECT\n",
    "          e.employee_id,\n",
    "          e.employee_name,\n",
    "          e.salary,\n",
    "          d.department_name\n",
    "    FROM \n",
    "          employee_info e\n",
    "    JOIN\n",
    "          avg_salaries a\n",
    "          ON e.department = a.department\n",
    "    JOIN \n",
    "          departments d\n",
    "          ON e.department = d.department_code\n",
    "    WHERE\n",
    "          e.salary > a.avg_salary\n",
    "    \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f75ba0",
   "metadata": {},
   "source": [
    "##### Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d409be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+--------+--------+-----------------+\n",
      "|VendorID|fare_amount|trip_count|min_fare|max_fare|         avg_fare|\n",
      "+--------+-----------+----------+--------+--------+-----------------+\n",
      "|       1|       47.8|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        5.1|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        8.6|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|       0.01|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|       44.3|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        6.5|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|       19.1|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        3.0|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        3.0|    827157|     0.0|   788.4|19.46594648899902|\n",
      "|       1|        3.0|    827157|     0.0|   788.4|19.46594648899902|\n",
      "+--------+-----------+----------+--------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregation\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          fare_amount,\n",
    "          COUNT(*) OVER (PARTITION BY VendorID) AS trip_count,\n",
    "          MIN(fare_amount) OVER (PARTITION BY VendorID) AS min_fare,\n",
    "          MAX(fare_amount) OVER (PARTITION BY VendorID) AS max_fare,\n",
    "          AVG(fare_amount) OVER (PARTITION BY VendorID) AS avg_fare\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "WHERE\n",
    "          VendorID = 1\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80721d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+-----------+-------+\n",
      "|VendorID|PULocationID|DOLocationID|fare_amount|row_num|\n",
      "+--------+------------+------------+-----------+-------+\n",
      "|       1|         132|         265|      788.4|      1|\n",
      "|       2|         132|         265|     1862.2|      1|\n",
      "|       6|         124|         223|        5.8|      1|\n",
      "|       2|          10|          10|      999.0|      2|\n",
      "|       1|         132|         265|      657.5|      2|\n",
      "|       6|          85|          77|        5.8|      2|\n",
      "|       2|         197|         197|      999.0|      3|\n",
      "|       1|         215|         265|      600.0|      3|\n",
      "|       6|         244|         223|        5.8|      3|\n",
      "|       2|          14|          14|      975.0|      4|\n",
      "+--------+------------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROW_NUMBER\n",
    "# WHERE clause is to show different VendorIDs, if needed\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          PULocationID,\n",
    "          DOLocationID,\n",
    "          fare_amount,\n",
    "          ROW_NUMBER() OVER (PARTITION BY VendorID ORDER BY fare_amount DESC) AS row_num\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "ORDER BY\n",
    "          row_num\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a0d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+-----------+----+----------+\n",
      "|VendorID|PULocationID|DOLocationID|fare_amount|rank|dense_rank|\n",
      "+--------+------------+------------+-----------+----+----------+\n",
      "|       1|         132|         265|      788.4|   1|         1|\n",
      "|       1|         132|         265|      657.5|   2|         2|\n",
      "|       1|         215|         265|      600.0|   3|         3|\n",
      "|       1|         132|         265|      600.0|   3|         3|\n",
      "|       1|         132|         265|      598.7|   5|         4|\n",
      "|       1|         145|         145|     500.55|   6|         5|\n",
      "|       1|          10|         265|      500.0|   7|         6|\n",
      "|       1|         264|         264|      500.0|   7|         6|\n",
      "|       1|          82|         265|      499.0|   9|         7|\n",
      "|       1|         265|         265|      434.0|  10|         8|\n",
      "+--------+------------+------------+-----------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANK: assign a rank based on a specific column and partition by another column, 1,2,2,4!,5\n",
    "# DENSE RANK: doesn't skip duplicates, 1,2,2,3,4\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          PULocationID,\n",
    "          DOLocationID,\n",
    "          fare_amount,\n",
    "          RANK() OVER (PARTITION BY VendorID ORDER BY fare_amount DESC) AS rank,\n",
    "          DENSE_RANK() OVER (PARTITION BY VendorID ORDER BY fare_amount DESC) AS dense_rank\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "WHERE\n",
    "          VendorID = 1\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55431d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+------------------+\n",
      "|VendorID|fare_amount|tpep_pickup_datetime|   cumulative_fare|\n",
      "+--------+-----------+--------------------+------------------+\n",
      "|       1|       24.0| 2024-09-01 00:00:01|              24.0|\n",
      "|       1|       24.7| 2024-09-01 00:00:04|              48.7|\n",
      "|       1|       63.9| 2024-09-01 00:00:06|             112.6|\n",
      "|       1|       12.1| 2024-09-01 00:00:09|             148.0|\n",
      "|       1|       23.3| 2024-09-01 00:00:09|             148.0|\n",
      "|       1|       28.9| 2024-09-01 00:00:19|            185.41|\n",
      "|       1|       8.51| 2024-09-01 00:00:19|            185.41|\n",
      "|       1|        7.9| 2024-09-01 00:00:22|            193.31|\n",
      "|       1|       12.1| 2024-09-01 00:00:30|            205.41|\n",
      "|       1|       10.7| 2024-09-01 00:00:32|216.10999999999999|\n",
      "+--------+-----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SUM in window functions: calculate cumulative sum of fare_amount by VendorID\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          fare_amount,\n",
    "          tpep_pickup_datetime,\n",
    "          SUM(fare_amount) OVER (PARTITION BY VendorID ORDER BY tpep_pickup_datetime) AS cumulative_fare\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ead9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+---------+\n",
      "|VendorID|fare_amount|prev_fare|next_fare|\n",
      "+--------+-----------+---------+---------+\n",
      "|       1|       24.0|     NULL|     24.7|\n",
      "|       1|       24.7|     24.0|     63.9|\n",
      "|       1|       63.9|     24.7|     12.1|\n",
      "|       1|       12.1|     63.9|     23.3|\n",
      "|       1|       23.3|     12.1|     28.9|\n",
      "|       1|       28.9|     23.3|     8.51|\n",
      "|       1|       8.51|     28.9|      7.9|\n",
      "|       1|        7.9|     8.51|     12.1|\n",
      "|       1|       12.1|      7.9|     10.7|\n",
      "|       1|       10.7|     12.1|      7.9|\n",
      "+--------+-----------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEAD and LAG: get the next and previous fare_amount by VendorID\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "          VendorID,\n",
    "          fare_amount,\n",
    "          LAG(fare_amount) OVER (PARTITION BY VendorID ORDER BY tpep_pickup_datetime) AS prev_fare,\n",
    "          LEAD(fare_amount) OVER (PARTITION BY VendorID ORDER BY tpep_pickup_datetime) AS next_fare\n",
    "FROM\n",
    "          taxi_2024_09\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104a745",
   "metadata": {},
   "source": [
    "#### SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609194d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACID\n",
    "\n",
    "# poetry add pandas\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('C:\\cubix_de/python_projects\\cubix_de_pyspark\\src\\cubix_de_pyspark\\data\\imdb_database.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72fdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atoma\\AppData\\Local\\Temp\\ipykernel_5700\\1367852744.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  title_basics = pd.read_csv('C:/cubix_de/python_projects/cubix_de_pyspark/src/cubix_de_pyspark/data/title.basics.tsv.gz', sep='\\t')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1518645"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create title.basics and title.ratings tables to our db\n",
    "\n",
    "title_basics = pd.read_csv('C:/cubix_de/python_projects/cubix_de_pyspark/src/cubix_de_pyspark/data/title.basics.tsv.gz', sep='\\t')\n",
    "title_basics.to_sql('title_basics', conn, if_exists='replace', index=False)\n",
    "\n",
    "title_ratings = pd.read_csv('C:/cubix_de/python_projects/cubix_de_pyspark/src/cubix_de_pyspark/data/title.ratings.tsv.gz', sep='\\t')\n",
    "title_ratings.to_sql('title_ratings', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0866c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tt0000001', 'short', 'Carmencita', 'Carmencita', '0', '1894', '\\\\N', '1', 'Documentary,Short')\n",
      "('tt0000002', 'short', 'Le clown et ses chiens', 'Le clown et ses chiens', '0', '1892', '\\\\N', '5', 'Animation,Short')\n",
      "('tt0000003', 'short', 'Poor Pierrot', 'Pauvre Pierrot', '0', '1892', '\\\\N', '5', 'Animation,Comedy,Romance')\n",
      "('tt0000004', 'short', 'Un bon bock', 'Un bon bock', '0', '1892', '\\\\N', '12', 'Animation,Short')\n",
      "('tt0000005', 'short', 'Blacksmith Scene', 'Blacksmith Scene', '0', '1893', '\\\\N', '1', 'Short')\n",
      "('tt0000006', 'short', 'Chinese Opium Den', 'Chinese Opium Den', '0', '1894', '\\\\N', '1', 'Short')\n",
      "('tt0000007', 'short', 'Corbett and Courtney Before the Kinetograph', 'Corbett and Courtney Before the Kinetograph', '0', '1894', '\\\\N', '1', 'Short,Sport')\n",
      "('tt0000008', 'short', 'Edison Kinetoscopic Record of a Sneeze', 'Edison Kinetoscopic Record of a Sneeze', '0', '1894', '\\\\N', '1', 'Documentary,Short')\n",
      "('tt0000009', 'movie', 'Miss Jerry', 'Miss Jerry', '0', '1894', '\\\\N', '45', 'Romance')\n",
      "('tt0000010', 'short', 'Leaving the Factory', \"La sortie de l'usine Lumière à Lyon\", '0', '1895', '\\\\N', '1', 'Documentary,Short')\n"
     ]
    }
   ],
   "source": [
    "# trditional way to get the first 10 rows of the title_basics table\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM title_basics LIMIT 10;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80738f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>short</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>short</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>5</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>short</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>5</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>short</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>12</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>short</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>0</td>\n",
       "      <td>1893</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tt0000006</td>\n",
       "      <td>short</td>\n",
       "      <td>Chinese Opium Den</td>\n",
       "      <td>Chinese Opium Den</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tt0000007</td>\n",
       "      <td>short</td>\n",
       "      <td>Corbett and Courtney Before the Kinetograph</td>\n",
       "      <td>Corbett and Courtney Before the Kinetograph</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Short,Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tt0000008</td>\n",
       "      <td>short</td>\n",
       "      <td>Edison Kinetoscopic Record of a Sneeze</td>\n",
       "      <td>Edison Kinetoscopic Record of a Sneeze</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0000009</td>\n",
       "      <td>movie</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>45</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tt0000010</td>\n",
       "      <td>short</td>\n",
       "      <td>Leaving the Factory</td>\n",
       "      <td>La sortie de l'usine Lumière à Lyon</td>\n",
       "      <td>0</td>\n",
       "      <td>1895</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst titleType                                 primaryTitle  \\\n",
       "0  tt0000001     short                                   Carmencita   \n",
       "1  tt0000002     short                       Le clown et ses chiens   \n",
       "2  tt0000003     short                                 Poor Pierrot   \n",
       "3  tt0000004     short                                  Un bon bock   \n",
       "4  tt0000005     short                             Blacksmith Scene   \n",
       "5  tt0000006     short                            Chinese Opium Den   \n",
       "6  tt0000007     short  Corbett and Courtney Before the Kinetograph   \n",
       "7  tt0000008     short       Edison Kinetoscopic Record of a Sneeze   \n",
       "8  tt0000009     movie                                   Miss Jerry   \n",
       "9  tt0000010     short                          Leaving the Factory   \n",
       "\n",
       "                                 originalTitle isAdult startYear endYear  \\\n",
       "0                                   Carmencita       0      1894      \\N   \n",
       "1                       Le clown et ses chiens       0      1892      \\N   \n",
       "2                               Pauvre Pierrot       0      1892      \\N   \n",
       "3                                  Un bon bock       0      1892      \\N   \n",
       "4                             Blacksmith Scene       0      1893      \\N   \n",
       "5                            Chinese Opium Den       0      1894      \\N   \n",
       "6  Corbett and Courtney Before the Kinetograph       0      1894      \\N   \n",
       "7       Edison Kinetoscopic Record of a Sneeze       0      1894      \\N   \n",
       "8                                   Miss Jerry       0      1894      \\N   \n",
       "9          La sortie de l'usine Lumière à Lyon       0      1895      \\N   \n",
       "\n",
       "  runtimeMinutes                    genres  \n",
       "0              1         Documentary,Short  \n",
       "1              5           Animation,Short  \n",
       "2              5  Animation,Comedy,Romance  \n",
       "3             12           Animation,Short  \n",
       "4              1                     Short  \n",
       "5              1                     Short  \n",
       "6              1               Short,Sport  \n",
       "7              1         Documentary,Short  \n",
       "8             45                   Romance  \n",
       "9              1         Documentary,Short  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas to fetch SQL results is not mthe most effective way to deal with large datasets, as the entire result of the query is fethed \n",
    "# from the db and loaded into memory, but good for exploration purposes\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM title_basics LIMIT 10;\", conn)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10acd0",
   "metadata": {},
   "source": [
    "#### Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubix-de-pyspark-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
